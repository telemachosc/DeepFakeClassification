{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Report.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNvmAfawFrmrMtbDaFYWHrl"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mXs5PftRidzS"},"source":["# Detecting Deep Fakes\n","### author: Telemachos Chatzitheodorou  \n","Total time to complete the task: 38 hours"]},{"cell_type":"markdown","metadata":{"id":"zx9GnuSbio9g"},"source":["## Table of Contents\n","\n","1.   [Introduction](#introduction)  \n","    1.1 [Face generation](#fg)  \n","    1.2 [Face alternation](#fa) \n","2. [How do you find which is real using Machine Learning?](#detectml)\n","3. [Solution Implementation](#solutionimplem)  \n","  3.1. [Autoencoder](#ae)  \n","  3.2  [Classification](#clas)  \n","   [Sources](#sources)\n"]},{"cell_type":"markdown","metadata":{"id":"ZxPsID52it1w"},"source":["## Introduction <a name=\"introduction\" />\n","\n","### Domain exploration.\n","\n","Deepfakes are AI generated images/videos with the ultimate purpose to provide   a realistic simulation of a scenario, although if build well enough to be undistinguisable to humans (and machines), they maybe used for nefarious ways [[1]](https://www.bbc.com/news/business-51204954). \n","The main uses are face generation, face alternation and facial attributes and expression.\n","\n","#### Face generation <a name=\"fa\" />\n","Although the today's SOTA algorithms are very good at face generation, luckily for us the are a few areas that fall short and a trained eye can have great accuracy in detecting them. \n","\n","The main areas face generated images are distinguishable from real ones are:\n","- artifacts in background  \n","<img src=\"https://graphics.reuters.com/CYBER-DEEPFAKE/ACTIVIST/nmovajgnxpa/img/background.jpg\" width=256/> <img src=\"https://cms.qz.com/wp-content/uploads/2018/12/earring2.png?quality=75&strip=all&w=620&h=638&crop=1\" height=256/>   \n","<sub>[*Source left*](https://graphics.reuters.com/CYBER-DEEPFAKE/ACTIVIST/nmovajgnxpa/), [*Source right*](https://qz.com/1510746/how-to-identify-fake-faces-generated-by-ai/)</sub>\n","\n","- teeth (fuzzy rendering, asymmetrical, improperly alligned)  \n","<img src=\"https://miro.medium.com/max/700/0*Wmh_X7JDjHho5d5V.png\" width=600/>    \n","<sub>[*Source*](https://arxiv.org/pdf/1912.04958.pdf)</sub>\n","\n","- eyes (non circular pupils)  \n","<img src=\"https://i.ibb.co/82pMQNX/pupils.png\" width=256/>  \n","<sub>[*Source*](#mdfc)</sub>\n","\n","- ears (asymmetric)  \n","- hair (fuzzy, not connecting)  \n","<img src=\"https://cms.qz.com/wp-content/uploads/2018/12/teeth3.png?quality=75&strip=all&w=620&h=626&crop=1\" width=256/>  \n","<sub>[*Source*](https://qz.com/1510746/how-to-identify-fake-faces-generated-by-ai/)</sub>\n","\n","- strange clothing  \n","<img src=\"https://cms.qz.com/wp-content/uploads/2018/12/background.png?quality=75&strip=all&w=620&h=614&crop=1\" width=256/>  \n","<sub>[*Source*](https://qz.com/1510746/how-to-identify-fake-faces-generated-by-ai/)</sub>\n","\n","\n","\n","The algorithms used for face generation are StyleGAN and StyleGAN2 which is the successor of StyleGAN and has resolved many issues from the first edition but the result is still not ideal.\n","\n","#### Face alteration <a name=\"fg\" />\n","\n","Face alteration can be applied to both photos and videos. In a photo you swap a face with another persons. In a video the process is more complicated because you have to match the facial expressions, lighting, shadows etc to be more realistic. Combine it with the proper audio dialog and you have the swapped person talking in the video.  \n","Luckily, the result has various flaws and most of the times you can distinguish such types of photos/videos, although the best submission in the Kaggle's Deepfake Detection challenge was at 65% accuracy [[2]](#mdfc) which shows how hard is to generate an automatic detection solution.  \n","Because the video part is far more vast than the photo one and out of scope for this project, I will present only some issues with deep fake photos that also apply in videos.  \n","Usually, the major faults in face alternation are:\n","- blur faces\n","- unatural skin tones  \n","<img src=\"https://miro.medium.com/max/700/1*w-ak0m1yxSVHS4eDHjWvtg.jpeg\" width=400/>  \n","<sub>[*Source*](#mdfc)</sub>\n","\n","- neglect face characteristics (longer/shorter forehead, double chin.. etc)  \n","<img src=\"https://miro.medium.com/max/700/1*NBbHYKkG6wqcvguDkFwI8Q.jpeg\" width=500/>  \n","<sub>[*Source*](#mdfc)</sub>\n","\n","- blurred boundaries on the edges of the face  \n","<img src=\"https://miro.medium.com/max/700/1*wyQasy3zWhzAeK24Kl8gQg.jpeg\" width=500/>  \n","<sub>[*Source*](#mdfc)</sub>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ijm0vlyUjwja"},"source":["## How do you find which is real using Machine Learning? <a name=\"detectml\" />\n","\n","To classify the real from fake images I will use a technique called feature extraction (with the use of the AutoEncoder architecture) and then use the encoder part to classify the given dataset using logistic regression\n","\n","### AutoEncoder\n","\n","The AutoEncoder network consists of two parts. The encoder and the decoder, as is shown in the picture below.  \n","\n","<img src=\"https://pythonmachinelearning.pro/wp-content/uploads/2017/10/Vanilla-Autoencoder.png.webp\" alt=\"autoencoder architecture\" />  \n","<sub>[*Source*](https://pythonmachinelearning.pro/all-about-autoencoders/)</sub>  \n","The input layer and output layer are the same size and the hidden layer is smaller than the size of the input and output layer. The hidden layer is compressed representation, the network learns two sets of weights (and biases) that encode input data into the compressed representation and decode the compressed representation back into input space. This type of autoencoder is also known as undercomplete because the compressed representation has less information than the input [[6]](#dl_book).\n","In order to evaluate how good the recostruction is I use the recostruction error, which is nothing more than the Euclidean distance loss $||x-\\hat{x}||^2$ aka Mean Squared Error.  \n","To train an AutoEncoder you don't need labeled data, so it is considered as an unsupervised learning method.  \n","The goal is to extract the minimum characteristics of a face (compressed representation) and use the encoder part as a pretrained part of a classifier. \n","\n","### Classification\n","Having trained the autoencoder, I will use only the encoder part which extracted the features of the training data and I will add an output layer for Logistic regression. The role of the layer is two-fold, it classifies the data and also fine tunes the network.  \n","\n","An illustration of the network is shown below.  \n","<img src=\"https://i.ibb.co/zhHLcJk/AE-LR.jpg\" alt=\"AE-LR\" border=\"0\"></a>  \n","<sub>[*Source*](#SDA)</sub>  \n","Because it's a supervised learning task, I need labeled data. For that task, we have two folders of fake and real images.\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H8jq5nv44pvQ"},"source":["## Solution Implementation <a name=\"solutionimplem\" />  "]},{"cell_type":"markdown","metadata":{"id":"tI1SCM27iu-l"},"source":["### Autoencoder <a name=\"ae\" />\n","#### Available data \n","First, let's examine the data we have at hand. Specifically the images in folder dataset1.\n","\n","I imported the dataset using tf.data.Dataset.\n","\n","<img src=\"https://i.ibb.co/h8FkTYW/data-eda.png\" alt=\"dataset\" />  \n","I see that the pixels values of the images range from 0 to 255.  I need to scale that range to [0., 1.] as machine learning models work better at that range due to the effective range of activation functions and bigger numbers tend to have greater impact on the output of a network. Due to the nature of the autoencoder, the normalization should be done to input and the outputs.\n","\n","#### Dataset Preperation\n","For the dataset I chose a split of 70%, 15%, 15% for train, validation and test sets.  \n","Also:\n","- batch number = 64\n","- shuffle with buffer size of 200\n","\n","The resulting datasets have shapes\n","\n","- train_ds: ((64, 256, 256, 3), (64, 256, 256, 3)) \n","- val_ds:  ((64, 256, 256, 3), (64, 256, 256, 3)) \n","- test_ds: ((64, 256, 256, 3), (64, 256, 256, 3))\n"," \n","The first element of the tuple corresponds to the input and the second to the output.  \n","\n","The number of samples for each one is:\n","- train_ds: 7000 \n","- val_ds: 1500 \n","- test_ds: 1500  \n","\n","The datasets are ready, but let's check if they are correctly assigning the same image to input and target.\n","<img src=\"https://i.ibb.co/prgY3f0/train-ds-input-target.png\" alt=\"input target dataset\" />  \n","The dataset seems to be built fine.  \n","Let's continue to the model building section.\n","\n","#### Model Implementation\n","For the implementation of the network it was used keras.\n","The network has 3 layers in the encoder and 4 in the decoder. The fourth layer of the decoder is added because the images have 3 channels.\n","\n","After copious testing the best results came with the following hyperparameters.\n","- Encoder\n","  - convolution 2d (filters=168, kernel=3, activation=relu, padding=same, strides=2)  \n","  - convolution 2d (filters=64, kernel=3, activation=relu, padding=same, strides=2)\n","  - convolution 2d (filters=32, kernel=3, activation= relu, padding=same, strides=2)\n","- Decoder\n","  - convolution 2d transpose (filters=32, kernel=3, activation= relu, padding=same, strides=2)  \n","  - convolution 2d transpose (filters=64, kernel=3, activation=relu, padding=same, strides=2)\n","  - convolution 2d transpose (filters=168, kernel=3, activation=relu, padding=same, stride=2)\n","  - convolution 2d (filters=3, kernel=3, activation=relu, padding=same)\n","\n","For the optimizer I used ADAM with learning rate at 0.001. In the bibliography is the SOTA for computer vision tasks [[5]](#https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/). For the loss, as stated above, the best choise is the Mean Squared Error. I also used accuracy as a more intuitive metric of the model's performance.\n","The model was trained for 10 epochs.  \n","The plot of the model is shown below  \n","<img src=\"https://i.ibb.co/52mtY90/ae.png\" />  \n","\n","#### Results\n","The training procedure can be seen in the image below (also available in the assignment.ipynb).  \n","<img src=\"https://i.ibb.co/WgtfhJw/training-ae.jpg\" />  \n","The plot for loss and validation loss   \n","<img src=\"https://i.ibb.co/ypkXszc/training-plot-ae.png\" />  \n","and accuracy  \n","<img src=\"https://i.ibb.co/VMW62K3/ae-accuracy.png\" />   \n","The model generalizes well as we can see from the test dataset\n","<img src=\"https://i.ibb.co/YP5S2yC/ae-evaluation.jpg\" />  \n","Also tried one image from the samples folder and the result is adequate.  \n","<ins>Input</ins>  \n","<img src=\"https://i.ibb.co/NS3LMDr/real-test-image.png\" />  \n","<ins>Output</ins>  \n","<img src=\"https://i.ibb.co/xMk3rHv/decoded-test-image.png\" />  \n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PuI5zftoJpv6"},"source":["### Classification <a name=\"clas\" />\n","\n","> **Note**: After investigation I discover that I have made some mistakes in that part that made my model untrainable.\n","> 1. The datasets were not balanced (fixed [here](https://shorturl.at/psyTW))\n","> 2. I mistakenly take the problem of classification as a transfer learning problem and used many layers in the classification part being influenced by other classification networks and consequently used incorrect activation functions.\n","> 3. The one hot encoding was integer encoding.\n","\n","For the classification task I follow the same procedure. Examine the data!\n","\n","The images have the same characteristics. The differences are going to be in the dataset preperation. This time I will use one-hot-encoding to encode the fake and real information as the target variable in each image. Also the normalization is going to be applied only on the input image.\n","\n","#### Dataset Preperation\n","\n","For the dataset I chose a split of 80%, 10%, 10% for train, validation and test sets as requested.  \n","I used 30% of the total number images which translates to 6000 images.  \n","Also:\n","- batch number = 128 (best results)\n","- shuffle with buffer size of 100 (applied only to the training set)\n","\n","The resulting datasets have shapes\n","\n","- train_ds_cal: ((128, 256, 256, 3), (128, 2))\n","- val_ds_cal:  ((128, 256, 256, 3), (128, 2))\n","- test_ds_cal: ((128, 256, 256, 3), (128, 2))\n","\n","and the number of samples per dataset is:\n","- train_ds_cal: 4800\n","- val_ds_cal:  600\n","- test_ds_cal: 600\n","\n","Now, let's see the data I am going to feed in the classifier.\n","\n","<img src=\"https://i.ibb.co/Gn4TyTK/real-fake-ds.png\" alt=\"input target dataset\" />  \n","The labeling seems correct. \n","\n","#### Model Implementation\n","As I mentioned above I will use the encoder part of the AutoEncoder model and add a logistic regression layer in the output.\n","\n","The architecture is as follows:\n","\n","- Encoder\n","- Flatten layer\n","- Dense layer (units=2, activation=softmax)\n","\n","The plot of the model is presented below  \n","\n","\n","The rest of the hyperparameters are:\n","- optimizer: ADAM (with learning rate at 0.001),\n","- loss: categorical cross entropy,\n","- metrics: accuracy, precision and recall.  \n","\n","The model was trained for 20 epochs.\n","\n","#### Results  \n","\n","The network was heavily underfitted so I added two regularization terms. The best combination are for:\n","- l1 = 0.01\n","- l2 = 0.1  \n","\n","Also tried the sigmoid activation function [[8]](#SDA) instead of softmax but the results were much worse.\n","\n","The training procedure is shown in the image below  \n","<a href=\"https://ibb.co/6JKyRqQ\"><img src=\"https://i.ibb.co/3YxTyPK/ae-lr-training-2.jpg\" alt=\"ae-lr-training-2\" border=\"0\"></a>\n","\n","also the loss,  \n","<img src=\"https://i.ibb.co/NF7Gcrc/ae-lr-loss.png\" alt=\"ae-lr-loss\" border=\"0\">\n","\n","the accuracy,  \n","<img src=\"https://i.ibb.co/nDtw9qG/ae-lr-accuracy.png\" alt=\"ae-lr-accuracy\" border=\"0\">\n","\n","the precision,  \n","<img src=\"https://i.ibb.co/nDtw9qG/ae-lr-accuracy.png\" alt=\"ae-lr-accuracy\" border=\"0\">\n","\n","and finally the recall.  \n","<img src=\"https://i.ibb.co/nDtw9qG/ae-lr-accuracy.png\" alt=\"ae-lr-accuracy\" border=\"0\">\n","\n","The generalization of the model is on par with the training results.\n","<img src=\"https://i.ibb.co/zZPv9QZ/ae-lr-evaluate.jpg\" alt=\"ae-lr-evaluate\" border=\"0\">\n","\n","Although the model seems to train well from the loss graph, the metrics indicate that there is room for improvement.\n","\n","#### Afterthoughts\n","I believe this is the best result with this configuration. Some possible points for achieving greater performance are:\n","- using a greater portion of the provided dataset.\n","- use of denoising autoencoders\n","- use a different train-validation-test split\n","----\n","\n","#### Optional subtask\n","\n","\n","For this task the only think I have to do is import the images from the samples folder, normalize them and add an extra dimension as the batch size.\n","\n","The images are shown below \n","<img src=\"https://i.ibb.co/rwdt1F3/download-1.png\" border=\"0\">\n","\n","\n","\n","with a little searching I found the actual labels for them which are:\n","\n","|  image number |  image label |  \n","|---------------|--------------|\n","|       1       |       real   |\n","|       2       |     fake     |\n","|       3       |     fake     |\n","|       4       |     fake     |\n","|       5       |     real     |\n","|       6       |     real     |\n","\n","The outputs of the model are probabilities. Passing the images from it, yields the following results\n","\n","    [[0.4371662  0.5628338 ]  \n","    [0.6060826  0.39391732]  \n","    [0.59835744 0.40164256]  \n","    [0.62500703 0.37499294]  \n","    [0.5071038  0.49289626]  \n","    [0.2657548  0.73424524]]\n","\n"," Each line correspond to one image and each element inside the line corresponds to the labels (fake, real).  \n"," So this array translates to the table below.\n","\n","|  image number |  predicted label| |  \n","|---------------|--------|-------- -| \n","|               | fake    | real    |\n","|               |       |           |\n","|       1       | 0.4371662 | 0.5628338|\n","|       2       | 0.6060826 | 0.39391732|\n","|       3       | 0.59835744 | 0.40164256 |\n","|       4       | 0.62500703 | 0.37499294 |\n","|       5       | 0.5071038 | 0.49289626 |\n","|       6       | 0.2657548 | 0.73424524 |\n","\n","\n","We see that the model performs adequately, given the training results."]},{"cell_type":"markdown","metadata":{"id":"Fr4sSInPivJE"},"source":["## Sources <a name=\"sources\" />\n","\n","1. [<a name=\"bbc\"> https://www.bbc.com/news/business-51204954</a>](https://www.bbc.com/news/business-51204954)\n","2. <a name=\"mdfc\"> https://jonathan-hui.medium.com/detect-ai-generated-images-deepfakes-part-1-b518ed5075f4</a>  \n","3. <a name=\"autoenc\"> https://pythonmachinelearning.pro/all-about-autoencoders/</a>  \n","4. <a name=\"dlbook\"> https://www.deeplearningbook.org/contents/autoencoders.html </a>\n","5. <a name=\"adam\"> https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/</a>\n","6. https://theaisummer.com/deepfakes/\n","7. https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/\n","8. <a name=\"SDA\"> C. Xing, L. Ma, and X. Yang, “Stacked Denoise Autoencoder Based Feature Extraction and Classification for Hyperspectral Images,” J. Sensors, vol. 2016, p. 3632943, 2016.</a>\n","9. Y. Chen, Z. Lin, X. Zhao, G. Wang, and Y. Gu, “Deep learning-based classification of hyperspectral data,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 2014.\n","\n","\n"]}]}